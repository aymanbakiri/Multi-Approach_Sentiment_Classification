\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{caption}
\usepackage{adjustbox} % Add this to your preamble
\usepackage{array}       % For advanced table features and column types
\usepackage[table]{xcolor}    % For table colors
\usepackage{multirow}    % For merging rows
\usepackage{makecell}    % For better cell formatting
\usepackage{booktabs}    % For professional table rules
\usepackage{ragged2e}
\usepackage{pdfpages}
\usepackage{placeins}
\usepackage{float}
\usepackage{titlesec}

% Use Roman numerals for section numbering
\renewcommand{\thesection}{\Roman{section}}

% Customize \section
\titleformat{\section}
  {\bfseries\Large} % Bold and Large font
  {\thesection.}    % Add Roman numeral as the prefix
  {0.5em}             % Space between prefix and title
  {}                % Code before the title

% Customize \subsection
\titleformat{\subsection}
  {\bfseries\large\itshape} % Bold, italicized, and slightly smaller font
  {}         % Add Arabic numeral as the prefix (e.g., 1.1.)
  {0.5em}                     % Space between prefix and title
  {}                        % Code before the title


\begin{document}
\title{Decoding Emotions: A Multi-Approach Sentiment Classification of Tweets}

\author{
Bakiri Ayman, Ben Mohamed Nizar, Chahed Ouazzani Adam\\
  \textit{CS-433 : Machine learning, EPFL}
}

\maketitle

\begin{abstract}
This project investigates sentiment classification of tweets, focusing on predicting whether a tweet conveyed positive :) or negative :( emotions based on its text. Multiple approaches were explored, ranging from traditional machine learning methods such as TF-IDF and GloVe embeddings combined with logistic regression, to advanced deep learning models including FastText, DistilBERT, and RoBERTa. 
The study highlights the importance of robust preprocessing, efficient hyperparameter tuning using Optuna, and the integration of context-aware models to improve classification performance. The results demonstrate the trade-offs between simplicity and accuracy, with advanced models like DistilBERT achieving high validation accuracy of 88.7\% and an F1 score of 88.9\%. Ethical concerns such as the misclassification of sarcasm and irony were addressed, ensuring the model is both effective and responsible. 
\end{abstract}


\section{Introduction}

The objective of this project is to develop a sentiment classification model to predict whether a tweet previously contained a positive :) or negative :( smiley, based on the remaining text. Our planned methodology involves starting with basic approaches, such as GloVe embeddings or TF-IDF combined with logistic regression. Subsequently, we aim to explore more advanced methods, including FastText, and ultimately experiment with deep neural networks such as DistilBERT, leveraging cluster resources for training. 


\section{Exploratory Data Analysis}



During the exploratory data analysis (\textbf{EDA}), we observed that the dataset is well-prepared for modeling, with no missing values and \textbf{balanced classes}. Each line in the dataset represents one tweet, making it straightforward to analyze.

We began by examining the \textbf{most common words, bigrams, and trigrams} in the positive and negative training data. Initially, many frequent terms such as "user," \texttt{<user>}, and even repetitive patterns like "user user user" were identified, alongside placeholders like "URL." These terms, while dominant, were deemed uninformative for sentiment classification. Hence, we removed them and reanalyzed the data for more meaningful insights.

Upon cleaning, the most common words in positive tweets included emotionally positive terms such as \textbf{"love," "good," "thanks," and "haha"}. In contrast, negative tweets highlighted terms like \textbf{"cant," "miss," and "really,"} reflecting more negative sentiments. Interestingly, some words such as "im" and "x" appeared frequently in both positive and negative contexts, indicating their \textbf{neutral or context-dependent nature}.

We also analyzed \textbf{hashtags}, which revealed insightful patterns. For instance, hashtags like \texttt{\#sadtweet} appeared predominantly in negative tweets, while others like \texttt{\#waystomakemehappy} were associated with positive contexts. 

Further, \textbf{bigram and trigram analysis} revealed context-rich patterns such as \textit{"thank you"} in positive tweets and \textit{"want to go"} in negative ones.


\section{Data Preprocessing}
\vspace{0.2cm} 

We employed light preprocessing for every method to remove irrelevant words identified during EDA, such as "user," "url," and "\textless user\textgreater"
 which were uninformative for sentiment classification. Removing these terms improved input quality and reduced noise, complementing the dataset's partially cleaned state.

Each method leveraged its own tokenization mechanism. Minimal preprocessing was applied for GloVe, as its embedding process handles text variability. In contrast, TF-IDF required additional steps to optimize feature representation.

\vspace{0.2cm} 
\subsubsection*{GloVe-Specific Preprocessing}
\vspace{0.2cm} 
No further preprocessing was conducted for GloVe embeddings. Tweets were directly converted into feature vectors using pre-trained embeddings, which can be downloaded following the instructions provided in the README.
\vspace{0.2cm} 
\subsubsection*{TF-IDF-Specific Preprocessing}
\vspace{0.2cm} 
To enhance feature representation for TF-IDF, we applied a more stringent preprocessing routine:
\begin{itemize}
    \item Removed stopwords using NLTK's stopword list.
    \item Removed punctuation and normalized text to lowercase.
    \item Removed repeating characters (e.g., "heeellooo" â†’ "hello") and numeric values.
    \item Tokenized the text into words.
    \item Applied stemming and lemmatization to reduce words to their base forms.
\end{itemize}
\vspace{0.2cm} 
\subsubsection*{FastText, DistilBERT and RoBERTA}

For advanced neural network-based methods, such as \textbf{FastText} and \textbf{DistilBERT}, minimal preprocessing was applied. These methods include their own tokenization processes:
\begin{itemize}
    \item \textbf{FastText}: This method works efficiently with words split into subwords and does not require removing rare tokens.
    \item \textbf{DistilBERT}: Initial experiments with extensive text preprocessing (e.g., removing stopwords, punctuation) led to poor performance with DistilBERT. This model inherently handle raw text effectively through it's tokenization mechanisms. Therefore, for these approaches, we decided to provide the \textbf{text data} directly to the tokenizer, retaining maximum contextual information.
\end{itemize}




\section{Methods}

\subsection{GloVe + Logistic Regression}
\label{sec:glove_logistic_regression}

To evaluate the effectiveness of GloVe embeddings in tweet sentiment classification, we utilized the pre-trained GloVe Twitter embeddings (glove.twitter.27B.100d.txt) with a Logistic Regression model from scikit-learn. The embeddings were first mapped to a 100-dimensional space, and the Logistic Regression model was trained with different regularization strengths using GridSearchCV, also from scikit-learn.


\subsubsection{Methodology}
Each tweet was represented as the mean of the GloVe word vectors for the tokens it contained. We employed a 5-fold cross-validation strategy, optimizing the regularization parameter $C$ for the Logistic Regression model. The tested values of $C$ ranged from $0.01$ to $10$, and the best model was determined based on cross-validated accuracy.

\subsubsection{Results}
The Logistic Regression model with GloVe embeddings achieved a highest validation accuracy of \textbf{76.2\%}, with the optimal regularization parameter being $C=1$. The Validation accuracy increased consistently as the regularization parameter grew from $0.01$ to $1$, plateauing beyond this point. 

Training accuracy remained slightly higher than validation accuracy, indicating minimal overfitting. 


To analyze the effect of embedding dimensions, we evaluated the Logistic Regression model with GloVe embeddings of different dimensions: 50, 100, and 200. The optimal regularization parameter \( C = 1 \), identified through GridSearchCV, was used for all configurations. Table~\ref{tab:glove_dimensions} summarizes the validation accuracy and F1 scores for each embedding dimension.

\begin{table}[H]
\centering
\caption{Performance of Logistic Regression with GloVe Embeddings Across Different Dimensions}
\label{tab:glove_dimensions}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Embedding Dimension} & \textbf{Validation Accuracy (\%)} & \textbf{F1 Score (\%)} \\ \midrule
50d                          & 74.5                             & 74.8                  \\
100d                         & 76.2                             & 76.3                  \\
200d                         & 77.6                             & 77.2                  \\ \bottomrule
\end{tabular}%
}
\end{table}

The results show that increasing the embedding dimension improves both validation accuracy and F1 score. The 200-dimensional embeddings achieved the highest performance, suggesting that larger embeddings better capture semantic nuances in the text.


\subsubsection{Discussion}
The results indicate that increasing the regularization parameter $C$ improves validation accuracy up to a certain point, stabilizing at $C=1$. Training accuracy remained slightly higher than validation accuracy, indicating minimal overfitting and good generalization.

Embedding dimension significantly impacted performance. The default 100-dimensional embeddings balanced accuracy and efficiency, while higher-dimensional embeddings (200d) further improved accuracy by capturing richer semantic information. However, this improvement comes at the cost of greater computational complexity.

Overall, the combination of GloVe embeddings and Logistic Regression was effective for sentiment classification, serving as a robust baseline for comparison with more complex models.


\subsection{TF-IDF + Logistic Regression}
\label{sec:tfidf_logistic_regression}

To evaluate the effectiveness of TF-IDF vectorization in tweet sentiment classification, we applied a Logistic Regression model using TF-IDF features. Each tweet was represented as a sparse vector generated by the TF-IDF transformation, capturing the importance of terms relative to the corpus.

\subsubsection{Methodology}
The Logistic Regression model was trained with different regularization strengths using GridSearchCV from scikit-learn. We conducted a 5-fold cross-validation to optimize the regularization parameter $C$, testing values ranging from $0.01$ to $10$. The model's performance was assessed based on cross-validated accuracy.

\subsubsection{Results}
The Logistic Regression model with TF-IDF features achieved a highest validation accuracy of \textbf{82.0\%}, with the best performance observed at a regularization parameter of $C=1$. The validation accuracy steadily improved as the regularization parameter increased from $0.01$ to $1$, stabilizing thereafter. 

Training accuracy followed a similar upward trend, remaining consistently higher than validation accuracy by a small margin, indicating that the model was well-regularized and generalized effectively to unseen data. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{tfidf.png}
    \caption{Performance of Logistic Regression with TF-IDF features. The x-axis represents the regularization parameter ($C$), while the y-axis indicates the accuracy. A log-scale is used for $C$.}
    \label{fig:tfidf_logistic_performance}
\end{figure}

\subsubsection{Discussion}
The results demonstrate that validation accuracy increases with larger values of the regularization parameter $C$, peaking at $C=1$, and stabilizing thereafter. The gap between training and validation accuracy is minimal, indicating that the model generalizes effectively.

The combination of TF-IDF vectorization and Logistic Regression provides a robust approach for tweet sentiment classification, delivering competitive accuracy and interpretability.

\subsubsection{Comparison Between GloVe and TF-IDF Representations}
TF-IDF outperformed GloVe embeddings in tweet sentiment classification, achieving higher validation accuracy across all regularization values. This suggests that TF-IDF's ability to capture corpus-specific term importance was more effective than the semantic relationships encoded in GloVe embeddings for this task. While GloVe offered competitive performance, its fixed, lower-dimensional representation may have limited its ability to fully capture dataset-specific nuances. These results highlight the importance of tailoring feature representations to the specific characteristics of the dataset.

\subsection{FastText}
\label{sec:fasttext}

\subsubsection{Methodology}
FastText, a word-embedding-based approach developed by Facebook AI, is highly efficient for text classification due to its use of subword embeddings and hierarchical softmax. We decided to switch from basic Logistic Regression to FastText as it captures subword information, making it more robust to spelling variations and rare words commonly found in tweets. To optimize its performance, we applied Optuna to tune the following hyperparameters:

\begin{itemize}
    \item \textbf{Learning Rate}: Log-uniform sampling between $0.001$ and $0.5$.
    \item \textbf{Epochs}: Integer range between $5$ and $100$.
    \item \textbf{Word N-grams}: Integer range between $1$ and $4$.
    \item \textbf{Embedding Dimensions}: Categorical choice of $\{50, 100, 200, 300\}$.
    \item \textbf{Loss Function}: Categorical choice of \texttt{softmax}, \texttt{hierarchical softmax (hs)}, and \texttt{negative sampling (ns)}.
\end{itemize}

\subsubsection{Results}
After hyperparameter tuning, the best-performing configuration achieved the following:
\begin{itemize}
    \item \textbf{Learning Rate}: $0.00347$
    \item \textbf{Epochs}: $69$
    \item \textbf{Word N-grams}: $4$
    \item \textbf{Embedding Dimensions}: $50$
    \item \textbf{Loss Function}: \texttt{softmax}
\end{itemize}

The model achieved:
\begin{itemize}
    \item \textbf{Validation Accuracy}: $83.9\%$
    \item \textbf{F1 Score}: $84.0\%$
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{output.png}
    \caption{Accuracy Scores Across Optuna Trials for FastText Hyperparameter Tuning}
    \label{fig:fasttext_optuna_trials}
\end{figure}

\subsubsection{Discussion}
FastText provided robust results, outperforming traditional models like Logistic Regression with GloVe embeddings. Its subword embeddings captured semantic relationships effectively. However, it fell slightly short of transformer-based models like DistilBERT and RoBERTa in terms of accuracy and F1 score. Despite this, its computational efficiency makes it a favorable choice for applications requiring low resource usage or rapid deployment.

\subsection{DistilBERT}
\label{sec:distilbert}

\subsubsection{Methodology}
DistilBERT, a distilled version of BERT, is a transformer-based model designed to retain 97\% of BERTâ€™s performance while being 60\% faster and requiring fewer computational resources. It achieves this by leveraging knowledge distillation during training, reducing the number of layers from 12 to 6 while maintaining the same hidden size and attention heads. DistilBERT was fine-tuned for sentiment classification using the Hugging Face \texttt{Trainer} API. The training process involved:

\begin{itemize}
    \item Tokenizing tweets with a maximum length of 128 using DistilBERT's tokenizer.
    \item Fine-tuning the pre-trained \texttt{distilbert-base-uncased} model on the tweet data.
    \item Optimizing hyperparameters via Optuna.
\end{itemize}

\subsubsection{Results}
The optimal configuration for DistilBERT achieved:
\begin{itemize}
    \item \textbf{Validation Accuracy}: $88.7\%$
    \item \textbf{F1 Score}: $88.9\%$
\end{itemize}
(Submission ID: 277867)

\subsubsection{Discussion}
DistilBERT offered a strong trade-off between accuracy and computational efficiency, achieving nearly the same performance as RoBERTa while being 60\% faster and lighter. Its contextual understanding of text through WordPiece tokenization contributed to superior handling of nuanced tweets. The minimal preprocessing requirement further streamlined its integration into the sentiment classification pipeline.

\subsection{Hyperparameter Tuning for DistilBERT}
\label{sec:distilbert_tuning}

Hyperparameter tuning for DistilBERT was conducted using Optuna. The search space included:
\begin{itemize}
    \item \textbf{Learning Rate}: Log-uniform sampling between $1 \times 10^{-5}$ and $5 \times 10^{-4}$.
    \item \textbf{Batch Size}: \{8, 16, 32\}.
    \item \textbf{Weight Decay}: Log-uniform sampling between 0.01 and 0.1.
    \item \textbf{Number of Epochs}: \{3 to 7\}.
\end{itemize}

The best hyperparameters identified were:
\begin{itemize}
    \item \textbf{Learning Rate}: $2 \times 10^{-5}$
    \item \textbf{Batch Size}: 16
    \item \textbf{Weight Decay}: 0.01
    \item \textbf{Number of Epochs}: 5
\end{itemize}

These hyperparameters allowed DistilBERT to achieve optimal results with validation accuracy and F1 scores of $88.7\%$ and $88.9\%$, respectively.

\begin{table}[H]
\centering
\caption{Hyperparameter Tuning Results for DistilBERT}
\label{tab:distilbert_tuning}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Learning Rate} & \textbf{Batch Size} & \textbf{Weight Decay} & \textbf{Epochs} & \textbf{Accuracy} & \textbf{F1 Score} \\ \midrule
$1 \times 10^{-5}$     & 8                   & 0.01                  & 3               & 87.1\%            & 87.3\%            \\
$3 \times 10^{-5}$     & 16                  & 0.05                  & 5               & 88.2\%            & 88.5\%            \\
$2 \times 10^{-5}$     & 16                  & 0.01                  & 5               & \textbf{88.7\%}   & \textbf{88.9\%}   \\
$5 \times 10^{-5}$     & 32                  & 0.1                   & 7               & 87.8\%            & 88.0\%            \\
$4 \times 10^{-5}$     & 8                   & 0.03                  & 6               & 88.0\%            & 88.2\%            \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{RoBERTa}
\label{sec:roberta}

\subsubsection{Methodology}
RoBERTa, an optimized variant of BERT, was fine-tuned for sentiment classification. The process included:
\begin{itemize}
    \item Tokenizing tweets to a maximum length of 128 using RoBERTa's tokenizer.
    \item Training the \texttt{roberta-base} model for 5 epochs with a learning rate of $3 \times 10^{-5}$ and batch sizes of 16 (training) and 32 (evaluation).
\end{itemize}

\subsubsection{Results}
The fine-tuned RoBERTa model achieved:
\begin{itemize}
    \item \textbf{Validation Accuracy}: $88.4\%$
    \item \textbf{F1 Score}: $88.7\%$
\end{itemize}
(Submission ID: 277552)

\subsubsection{Discussion}
RoBERTa demonstrated high performance, comparable to DistilBERT, but required significantly more computational resources. Its improved pre-training strategy and larger dataset usage enhanced its contextual understanding, making it effective for nuanced tweet analysis. However, the increased training time may limit its applicability in time-sensitive scenarios.



\subsection{Comparison Between FastText, DistilBERT, and RoBERTa}
\label{sec:comparison}

\begin{itemize}
    \item \textbf{Accuracy and F1 Score}: DistilBERT achieved the highest validation accuracy of $88.7\%$, outperforming FastText ($83.9\%$). RoBERTa matched DistilBERT but at a higher computational cost.
    \item \textbf{Efficiency}: FastText required significantly less training time and resources than transformer-based models.
    \item \textbf{Complexity Handling}: DistilBERT and RoBERTa excelled in capturing context and nuances, outperforming FastText on ambiguous or sarcastic tweets.
    \item \textbf{Resource Constraints}: FastText is ideal for lightweight tasks, while DistilBERT offers a balance of accuracy and efficiency. RoBERTa is suited for maximum accuracy regardless of resources.
\end{itemize}

Overall, DistilBERT provides the best trade-off between accuracy, efficiency, and resource usage, making it suitable for most applications. FastText excels in resource-constrained scenarios, while RoBERTa is best for tasks demanding maximum accuracy.


\section{Scientific Novelty}
This project addresses the challenges of sentiment classification in short and informal texts like tweets. These challenges include:
\begin{itemize}
    \item \textbf{Ambiguity and Sarcasm}: Tweets often lack context, making it difficult for traditional models to understand nuanced expressions.
    \item \textbf{Noise in Text}: Tweets contain informal language, abbreviations, and symbols that complicate feature extraction.
\end{itemize}

Our approach introduced several innovations to tackle these issues:
\begin{itemize}
    \item Leveraging \textbf{Optuna for Hyperparameter Tuning} ensured optimal configurations for all models, significantly improving performance.
    \item Employing \textbf{context-aware models} (DistilBERT and RoBERTa) enabled better handling of ambiguity and sarcasm.
    \item Demonstrating the \textbf{trade-offs between traditional and deep learning methods}, highlighting the strengths of TF-IDF for interpretable feature selection and the power of transformers for semantic understanding.
\end{itemize}



\section{Conclusion}
In this project, we explored various approaches for sentiment classification of tweets, from traditional models like Logistic Regression with TF-IDF and GloVe embeddings to advanced deep learning models like FastText, DistilBERT, and RoBERTa.

The results highlight trade-offs between simplicity and complexity. TF-IDF and GloVe provided interpretable and efficient solutions, while DistilBERT achieved state-of-the-art performance with 88.7\% validation accuracy and an 88.9\% F1 score. FastText offered a good balance between simplicity and performance, outperforming traditional methods while maintaining efficiency.

Key factors included robust preprocessing, hyperparameter tuning with Optuna, and context-aware models for capturing semantic nuances. Future work could address sarcasm and contextual ambiguity, potentially leveraging multimodal data like images or user metadata to enhance classification.


\newpage

\subsection{Ethical Concerns}

\subsubsection{Overview of Risks}
A significant risk in sentiment classification is the misclassification of tweets, particularly when the sentiment conveyed by smileys conflicts with the overall context or intent of the text. Examples include:
\begin{itemize}
    \item \textbf{Sarcasm and Irony}: Tweets such as \textit{``Oh great, everything's ruined :)''}, were often misclassified as positive due to the presence of a smiley, despite the underlying negative sentiment.
    \item \textbf{Ambiguity}: Tweets like \textit{``I can't believe it worked!''} depend on context and were sometimes misclassified by traditional models like Logistic Regression.
    \item \textbf{Bias in Data}: Certain hashtags and phrases disproportionately influenced predictions due to their prevalence in the training data, potentially amplifying biases.
\end{itemize}

\subsubsection{Impact of Misclassification}
The misclassification of tweets can negatively affect:
\begin{itemize}
    \item \textbf{Content Creators}: Misinterpretation of the author's intent can lead to misrepresentation in sentiment-based analytics.
    \item \textbf{End-Users}: Applications such as content moderation, sentiment tracking, or recommendations may produce misleading results, reducing user trust.
\end{itemize}

\subsubsection{Evaluation and Metrics}
To address these risks, we relied on the adoption of context-aware models (e.g., DistilBERT and RoBERTa) to better handle nuanced contexts like sarcasm and ambiguity. These models showed significant performance improvements over traditional approaches, as shown in Table~\ref{tab:ethical_improvements}.

\begin{table}[H]
    \centering
    \caption{Performance Improvement with Context-Aware Models}
    \label{tab:ethical_improvements}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    \textbf{Model}              & \textbf{Accuracy (\%)} & \textbf{F1 Score (\%)} \\ \midrule
    Logistic Regression (TF-IDF) & 82.0                   & 82.0                   \\
    DistilBERT                   & 88.7                   & 88.9                   \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Mitigation Strategies}
To minimize misclassification risks, the following strategies were employed:
\begin{itemize}
    \item \textbf{Advanced Models}: Context-aware models like DistilBERT and RoBERTa inherently address risks related to sarcasm and ambiguity by capturing semantic and contextual nuances.
    \item \textbf{Quantitative Validation}: Improvements in accuracy and F1 provided strong evidence of reduced misclassification rates.
\end{itemize}

\subsubsection{Conclusion}
The adoption of context-aware models mitigated many ethical risks by improving performance on nuanced text. Future work could explore multi-modal sentiment analysis, incorporating additional context from images or metadata to further enhance robustness.


\clearpage
\appendix
\addcontentsline{toc}{section}{Appendix}

\section{Appendix}

\subsection{References}

\begin{itemize}
    \item \textbf{Scikit-learn}:  
    Pedregosa et al., "Scikit-learn: Machine Learning in Python", Journal of Machine Learning Research, 2011.  
    [Online]. Available: \url{https://scikit-learn.org/stable/}
    
    \item \textbf{FastText}:  
    Grave et al., "Learning Word Vectors for Sentiment Analysis", Facebook AI Research, 2017.  
    [Online]. Available: \url{https://fasttext.cc/}
    
    \item \textbf{Optuna}:  
    Akiba et al., "Optuna: A Next-generation Hyperparameter Optimization Framework", 2019.  
    [Online]. Available: \url{https://optuna.org/}
    
    \item \textbf{Transformers Library}:  
    Wolf et al., "Transformers: State-of-the-Art Natural Language Processing", 2020.  
    [Online]. Available: \url{https://huggingface.co/transformers/}
    
    \item \textbf{GloVe Embeddings}:  
    Pennington et al., "GloVe: Global Vectors for Word Representation", 2014.  
    [Online]. Available: \url{https://nlp.stanford.edu/projects/glove/}
    
    \item \textbf{Datasets Library}:  
    Hugging Face, "Datasets: A Community Library for Natural Language Processing", 2020.  
    [Online]. Available: \url{https://huggingface.co/docs/datasets/}
    
    \item \textbf{NLTK (Natural Language Toolkit)}:  
    Bird et al., "NLTK: The Natural Language Toolkit", 2009.  
    [Online]. Available: \url{https://www.nltk.org/}
\end{itemize}

\includepdf[pages=-]{flatcanva.pdf}

\end{document}
